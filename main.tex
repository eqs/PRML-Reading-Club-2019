\documentclass[10pt]{beamer}

\usepackage{amsmath,amssymb}
\usepackage{zxjatype}
\usepackage[ipa]{zxjafont}
\usetheme{metropolis}
\usepackage{tikz}
\usepackage{tikzsymbols}
\usepackage{appendixnumberbeamer}
\usepackage{bm}

\usefonttheme{professionalfonts}
\usetikzlibrary{positioning}

% -- block color ---
\setbeamercolor{block title}{use=structure, fg=white!90!purple, bg=purple!75!black}
\setbeamercolor{block body}{use=structure, fg=black!90!white, bg=white!90!black}
\setbeamercolor{block title example}{use=structure, fg=white!90!cyan, bg=cyan!75!black}
\setbeamercolor{block body example}{use=structure, fg=black!90!white, bg=white!90!black}

% --- page number ---
\setbeamertemplate{footline}{%
	\raisebox{10pt}{\makebox[\paperwidth]{\hfill\makebox[7em]{\normalsize\texttt{\insertframenumber/\inserttotalframenumber}}}}%
}

% --- title logo ---

\newcommand{\myinsertlogo}[1]{%
\begin{tikzpicture}[overlay, remember picture]
    \node[above left=1cm and .8cm of current page.south east] {\includegraphics[width=2.25cm]{#1}};
\end{tikzpicture}}

% --- commands ---

\newcommand{\redtext}[1]{\textcolor{red}{#1}}
\newcommand{\bluetext}[1]{\textcolor{blue}{#1}}
\newcommand<>{\greentext}[1]{\alt#2{\textcolor{green}{#1}}{#1}}
\newcommand<>{\highlight}[2][yellow]{%
    \alt#3{%
        \tikz[baseline=(x.base)]{
            \node[rectangle,rounded corners,fill=#1!10](x){$#2$};
        }%
    }{%
        #2
    }%
}

\newcommand<>{\highlightcap}[3][yellow]{%
    \alt#4{%
        \tikz[baseline=(x.base)]{
            \node[rectangle,rounded corners,fill=#1!10](x){$#2$};
            \node[anchor=north, color=#1, align=center] at (x.south) {#3};
        }%
    }{%
        #2
    }%
}

\newcommand<>{\highlightcaphead}[3][yellow]{%
    \alt#4{%
        \tikz[baseline=(x.base)]{
            \node[rectangle,rounded corners,fill=#1!10](x){$#2$};
            \node[anchor=south, color=#1, align=center] at (x.north) {#3};
        }%
    }{%
        #2
    }%
}

\newenvironment{supframe}[1]{%
    \setbeamercolor{frametitle}{fg=white!90!purple, bg=mLightGreen!75!black}%
    \begin{frame}{#1}%
}{%
    \end{frame}%
}

\newcommand{\nbracket}[1]{\left( #1 \right)}
\newcommand{\cbracket}[1]{\left\{ #1 \right\}}
\newcommand{\rbracket}[1]{\left[ #1 \right]}
\newcommand{\abracket}[1]{\left\langle #1 \right\rangle}

\DeclareMathOperator{\tr}{tr}

\title{Probability Distributions (PRML \S2.3.1-2.3.7)}
\date{PRML Reading Club (June 3, 2019)}
\author{Satoshi Murashige}
\institute{Mathematical Informatics Lab., NAIST}

\begin{document}
    \begin{frame}[plain]
        \maketitle
        \myinsertlogo{naist.pdf}
    \end{frame}
    
    \begin{frame}{Table of Contents}
        \begin{itemize}
            \item Important properties of Gaussian distributions
                \begin{itemize}
                    \item \S2.3.1 Conditional Gaussian Distribution
                    \item \S2.3.2 Marginal Gaussian Distribution
                    \item \S2.3.3 Bayes' theorem for Gaussian variables
                \end{itemize}
            \item Parameter estimation for the Gaussian
                \begin{itemize}
                    \item \S2.3.4 Maximum likelihood for the Gaussian
                    \item \S2.3.5 Sequential estimation
                    \item \S2.3.6 Bayesian inference for the Gaussian
                \end{itemize}
            \item Students's t-distribution
                \begin{itemize}
                    \item \S2.3.7 Students's t-distribution
                \end{itemize}
        \end{itemize}
    \end{frame}
    
    \begin{frame}{Completing the square (for univariate case)}
        \begin{itemize}
            \item Basic idea:
                \begin{align*}
                    &x^2 - 6x + 2 \\
                    \onslide<2->{&= \highlight<3->[red]{x^2 - 6x + 9} - 7 \\}
                    \onslide<4->{&= \highlight[red]{(x - 3)^2} - 7}
                \end{align*}
            \item Example:
                \begin{align*}
                    &\exp\left(-\frac{1}{2}x^2 + 2x + \mathrm{const.}\right) \\
                    \onslide<5->{&= \exp\left\{-\frac{1}{2}(x^2 - 4x + 4) + \mathrm{const.}\right\} \\}
                    \onslide<6->{&= \highlightcap<7->[red]{\exp\left\{-\dfrac{(x - 2)^2}{2}\right\}}{\small Unnormalized Gaussian}\cdot\exp(\mathrm{const.}) }
                \end{align*}
        \end{itemize}
    \end{frame}
    
    \section{\S2.3.1 Conditional Gaussian Distribution}
    
    \begin{frame}{Important properties of Gaussian Distribution}
        Now, we consider to derive the following properties:
        \begin{align*}
            &p(\mathbf x_a, \mathbf x_b) = p(\mathbf x) = \mathcal N(\mathbf x | \bm \mu, \bm \Sigma) 
            \Rightarrow 
            \begin{array}{l}
                 p(\mathbf x_a | \mathbf x_b) = \mathcal N(\mathbf x_a | \bm \mu_{a|b}, \bm \Sigma_{a|b})  \\
                 p(\mathbf x_a) = \mathcal N(\mathbf x_a | \bm \mu_a, \bm \Sigma_a) 
            \end{array}
        \end{align*}
        \begin{center}
            \includegraphics[width=0.45\hsize]{figs/Figure2_9a.pdf}
            \hfill
            \includegraphics[width=0.45\hsize]{figs/Figure2_9b.pdf} \\
            Figure 2.9
        \end{center}
    \end{frame}
    
    \begin{frame}{Definition of Notation (1/2)}
        Consider a joint distribution $p(\mathbf x) = \mathcal N(\mathbf x | \bm \mu, \bm \Sigma)$
        \begin{itemize}
            \item Separate a $D$-dimensional vector $\mathbf x \sim \mathcal N(\mathbf x | \bm \mu, \bm \Sigma)$
                into $\mathbf x_a \in \mathbb R^M$ and $\mathbf x_b \in \mathbb R^{D-M}$
                \begin{align*}
                    \mathbf x &= \begin{pmatrix}
                        \mathbf x_a \\
                        \mathbf x_b 
                    \end{pmatrix} \tag{2.65}\\
                    \bm \mu &= \begin{pmatrix}
                        \bm \mu_a \\
                        \bm \mu_b 
                    \end{pmatrix} \tag{2.66}\\
                    \bm \Sigma &= \begin{pmatrix}
                        \bm \Sigma_{aa} & \bm \Sigma_{ab} \\
                        \bm \Sigma_{ba} & \bm \Sigma_{bb} 
                    \end{pmatrix} \tag{2.67}
                \end{align*}
            \item The symmetry $\bm\Sigma^\top = \bm\Sigma$ of the covariance matrix, 
                \begin{align*}
                    \bm\Sigma_{aa}^\top = \bm\Sigma_{aa}&, \bm\Sigma_{bb}^\top = \bm\Sigma_{bb} \;\; (\mbox{i.e. symmetry})\\
                    \bm\Sigma_{ba}^\top &= \bm\Sigma_{ab}
                \end{align*}
        \end{itemize}
    \end{frame}

    \begin{frame}{Definition of Notation (2/2)}
        \begin{itemize}
            \item The \textit{precision matrix} $\bm\Lambda$ is convenient in many situations
                \begin{align*}
                    \bm\Lambda &\equiv \bm\Sigma^{-1} \tag{2.68} \\
                    \bm\Lambda &= \begin{pmatrix}
                        \bm \Lambda_{aa} & \bm \Lambda_{ab} \\
                        \bm \Lambda_{ba} & \bm \Lambda_{bb} 
                    \end{pmatrix} \tag{2.69}
                \end{align*}
            \item Because the inverse of a symmetric matrix is also symmetric (proof: ex. 2.22), 
                \begin{align*}
                    \bm\Lambda_{aa}^\top = \bm\Lambda_{aa}&, \bm\Lambda_{bb}^\top = \bm\Lambda_{bb} \;\; (\mbox{i.e. symmetry})\\
                    \bm\Lambda_{ba}^\top &= \bm\Lambda_{ab}
                \end{align*}
            \item NOTE: Generally, for instance, $\bm\Lambda_{aa} \not = \bm\Sigma_{aa}^{-1}$
        \end{itemize}
    \end{frame}
    
    \begin{supframe}{Solution of Exercise 2.22}
        Let $\mathbf A$ be a symmetric matrix ($\mathbf A = \mathbf A^\top$).
        The inverse matrix $\mathbf A^{-1}$ satisfies
        \[
            \mathbf A\mathbf A^{-1} = \mathbf I.
        \]
        By taking the transpose of both sides of this equation, 
        we obtain
        \[
            (\mathbf A^{-1})^\top \mathbf A^\top = \mathbf I.
        \]
        From the definition of inverse matrix, we obtain
        \[
            (\mathbf A^{-1})^\top = \mathbf A^{-1}.
        \]
        Therefore, $\mathbf A^{-1}$ is also symmetric matrix.
    \end{supframe}
    
    \begin{frame}{Derivation of Conditional Gaussian Distribution}
        \begin{block}{Conditional Gaussian Distribution}
            \[
                p(\mathbf x_a, \mathbf x_b) = \mathcal N(\mathbf x | \bm\mu, \bm\Sigma)
                \Rightarrow p(\mathbf x_a | \mathbf x_b) = \mathcal N(\mathbf x_a | \bm\mu_{a|b}, \bm\Sigma_{a|b})
            \]
        \end{block}
        From the product rule of probability, 
        \begin{align*}
            \highlightcap<2->[blue]{p(\mathbf x_a | \mathbf x_b)}{Unknown}
            = \frac{\highlightcaphead<2->[red]{p(\mathbf x_a, \mathbf x_b)}{Known}}{\highlightcap<2->[blue]{p(\mathbf x_b)}{Unknown}}
        \end{align*}
        Take the logarithm of both sides, 
        \begin{align*}
            \ln p(\mathbf x_a | \mathbf x_b) 
            &= \ln p(\mathbf x_a, \mathbf x_b) + \mathrm{const.} \\
            &= -\frac{1}{2}(\mathbf x - \bm\mu)^\top\bm\Sigma^{-1}(\mathbf x - \bm\mu) + \mathrm{const.}
        \end{align*}

    \end{frame}
    
    \begin{frame}{Derivation of Conditional Gaussian Distribution}
        Because (2.70) is a quadratic form of $\mathbf x_a$,
        the corresponding conditional distribution $p(\mathbf x_a | \mathbf x_b)$ will be Gaussian.
        \begin{align*}
            &-\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu) \\
            &=-\frac{1}{2}(\mathbf x_a - \bm\mu_a)^\top \bm\Lambda_{aa}(\mathbf x_a - \bm\mu_a)
            -\frac{1}{2}(\mathbf x_a - \bm\mu_a)^\top \bm\Lambda_{ab}(\mathbf x_b - \bm\mu_b) \\
            &\qquad -\frac{1}{2}(\mathbf x_b - \bm\mu_b)^\top \bm\Lambda_{ba}(\mathbf x_a - \bm\mu_a)
            -\frac{1}{2}(\mathbf x_b - \bm\mu_b)^\top \bm\Lambda_{bb}(\mathbf x_b - \bm\mu_b) \tag{2.70} \\
            %---------------
            \onslide<2->{
            &= -\frac{1}{2}\left( \highlight[red]{\mathbf x_a^\top\bm\Lambda_{aa}\mathbf x_a} - \highlight[red]{\mathbf x_a^\top\bm\Lambda_{aa}\bm\mu_a} - 
                               \highlight[red]{\bm\mu_a^\top\bm\Lambda_{aa}\mathbf x_a} + \bm\mu_a^\top\bm\Lambda_{aa}\bm\mu_a \right) \\
            &\qquad -\frac{1}{2}\left( \mathbf x_a^\top\bm\Lambda_{ab}\mathbf x_b - \highlight[red]{\mathbf x_a^\top\bm\Lambda_{ab}\bm\mu_b} - 
                               \bm\mu_a^\top\bm\Lambda_{ab}\mathbf x_b + \bm\mu_a^\top\bm\Lambda_{ab}\bm\mu_b \right) \\
            &\qquad -\frac{1}{2}\left( \mathbf x_b^\top\bm\Lambda_{ba}\mathbf x_a - \mathbf x_b^\top\bm\Lambda_{ba}\bm\mu_a - 
                               \highlight[red]{\bm\mu_b^\top\bm\Lambda_{ba}\mathbf x_a} + \bm\mu_b^\top\bm\Lambda_{ba}\bm\mu_a \right) \\
            &\qquad -\frac{1}{2}\left( \mathbf x_b^\top\bm\Lambda_{bb}\mathbf x_b - \mathbf x_b^\top\bm\Lambda_{bb}\bm\mu_b - 
                               \bm\mu_b^\top\bm\Lambda_{bb}\mathbf x_b + \bm\mu_b^\top\bm\Lambda_{bb}\bm\mu_b \right) \\
            &= \highlightcap[red]{\displaystyle -\frac{1}{2}\mathbf x_a^\top\bm\Lambda_{aa}\mathbf x_a}{\small The second order term}
                + \highlightcap[red]{\mathbf x_a^\top(\bm\Lambda_{aa}\bm\mu_a + \bm\Lambda_{ab}\bm\mu_b)}{The linear term}
                + \mathrm{const.}
            }
        \end{align*}
    \end{frame}

    \begin{frame}{Derivation of Conditional Gaussian Distribution}
        \begin{block}{Completing the square}
            \begin{align*} -\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu)
                = -\frac{1}{2}\mathbf x^\top\bm\Sigma^{-1}\mathbf x + \mathbf x^\top\bm\Sigma^{-1}\bm\mu + \mathrm{const.} \tag{2.71}
            \end{align*}
        \end{block}\vspace{-5mm}
        \begin{align*}
            &-\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu) \\
            &= -\frac{1}{2}\mathbf x_a^\top\bm\Lambda_{aa}\mathbf x_a
                + \mathbf x_a^\top(\bm\Lambda_{aa}\bm\mu_a + \bm\Lambda_{ab}\bm\mu_b)
                + \mathrm{const.} \\
            \onslide<2->{
                &=
                -\frac{1}{2}\mathbf x_a^\top\highlightcap<3->[red]{\bm\Lambda_{aa}}{$\bm\Sigma^{-1}$}\mathbf x_a
                    + \mathbf x_a^\top \highlightcap<3->[red]{\bm\Lambda_{aa}}{$\bm\Sigma^{-1}$}
                    \highlightcap<3->[blue]{\bm\Lambda_{aa}^{-1} (\bm\Lambda_{aa}\bm\mu_a + \bm\Lambda_{ab}\bm\mu_b)}{$\bm\mu$}
                    + \mathrm{const.} \\
            }
            \onslide<4->{
                &= -\frac{1}{2}(\mathbf x_a - \bm\mu_{a|b})^\top \bm\Sigma_{a|b}^{-1}(\mathbf x_a - \bm\mu_{a|b}) + \mathrm{const.}
            }
        \end{align*}\vspace{-4mm}
        \onslide<5->{
            \begin{align*}
                \bm\Sigma_{a|b} &= \bm\Lambda_{aa}^{-1} \tag{2.73}\\
                \bm\mu_{a|b} &= \bm\mu_a - \bm\Lambda_{aa}^{-1}\bm\Lambda_{ab}(\mathbf x_b - \bm\mu_b) \tag{2.75}
            \end{align*}
        }
    \end{frame}

    \begin{frame}{Derivation of Conditional Gaussian Distribution}
        Therefore, 
        \begin{align*}
            \ln p(\mathbf x_a | \mathbf x_b) 
            &= -\frac{1}{2}(\mathbf x_a - \bm\mu_{a|b})^\top \bm\Sigma_{a|b}^{-1}(\mathbf x_a - \bm\mu_{a|b}) + \mathrm{const.}
        \end{align*}
        \begin{align*}
            \therefore \; & p(\mathbf x_a | \mathbf x_b) 
            \propto \exp\left\{-\frac{1}{2}(\mathbf x_a - \bm\mu_{a|b})^\top \bm\Sigma_{a|b}^{-1}(\mathbf x_a - \bm\mu_{a|b}) \right\} \\
            & \xrightarrow{\mathrm{Normalization}} p(\mathbf x_a | \mathbf x_b) = \mathcal N(\mathbf x_a | \bm\mu_{a|b}, \bm\Sigma_{a|b})
        \end{align*}
        \begin{align*}
            \bm\Sigma_{a|b} &= \bm\Lambda_{aa}^{-1} \tag{2.73}\\
            \bm\mu_{a|b} &= \bm\mu_a - \bm\Lambda_{aa}^{-1}\bm\Lambda_{ab}(\mathbf x_b - \bm\mu_b) \tag{2.75}
        \end{align*}
    \end{frame}

    \begin{frame}{Express the results in terms of the covariance matrix}
        \begin{itemize}
            \item Recall:
                \begin{align*}
                    p(\mathbf x_a | \mathbf x_b) &= \mathcal N(\mathbf x_a | \bm\mu_{a|b}, \bm\Sigma_{a|b}) \\
                    \bm\Sigma_{a|b} &= \bm\Lambda_{aa}^{-1} \tag{2.73}\\
                    \bm\mu_{a|b} &= \bm\mu_a - \bm\Lambda_{aa}^{-1}\bm\Lambda_{ab}(\mathbf x_b - \bm\mu_b) \tag{2.75}
                \end{align*}
            \item The results (2.73) and (2.75) are expressed in terms of the partitioned precision matrix.
            \item We can also express these results in terms of the corresponding partitioned covariance matrix.
                \begin{align*}
                    \begin{pmatrix}
                        \bm \Sigma_{aa} & \bm \Sigma_{ab} \\
                        \bm \Sigma_{ba} & \bm \Sigma_{bb} 
                    \end{pmatrix}^{-1} =
                    \begin{pmatrix}
                        \bm \Lambda_{aa} & \bm \Lambda_{ab} \\
                        \bm \Lambda_{ba} & \bm \Lambda_{bb} 
                    \end{pmatrix} \tag{2.78}
                \end{align*}
        \end{itemize}
        \note{
            ここまでの議論で導かれた結果は精度行列の言葉で表現したものです．
            一方，我々はこれを元の共分散行列の言葉でも表現することが可能です．
        }
    \end{frame}

    \begin{frame}{Express the results in terms of the covariance matrix}
        \begin{block}{The identity for the inverse of a partitioned matrix (Exercise 2.24)}
            \begin{align*}
                \begin{pmatrix}
                    \mathbf A & \mathbf B \\
                    \mathbf C & \mathbf D
                \end{pmatrix}^{-1} &= 
                \begin{pmatrix}
                    \mathbf M & -\mathbf M\mathbf B\mathbf D^{-1} \\
                    -\mathbf D^{-1}\mathbf C\mathbf M & \mathbf D^{-1}+\mathbf D^{-1}\mathbf C\mathbf M \mathbf B \mathbf D^{-1}
                \end{pmatrix} \tag{2.76}\\
                \mathbf M &= (\mathbf A - \mathbf B\mathbf D^{-1}\mathbf C)^{-1} \tag{2.77}
            \end{align*}
        \end{block}
        \vspace{-0.5cm}
        \begin{align*}
            \begin{pmatrix}
                \bm \Sigma_{aa} & \bm \Sigma_{ab} \\
                \bm \Sigma_{ba} & \bm \Sigma_{bb} 
            \end{pmatrix}^{-1} =
            \begin{pmatrix}
                \bm \Lambda_{aa} & \bm \Lambda_{ab} \\
                \bm \Lambda_{ba} & \bm \Lambda_{bb} 
            \end{pmatrix} \tag{2.78}
        \end{align*}
        Using (2.76), (2.77) and (2.78), we have
        \begin{align*}
            \bm\Lambda_{aa} &= (\bm\Sigma_{aa} - \bm\Sigma_{ab}\bm\Sigma_{bb}^{-1}\bm\Sigma_{ba})^{-1} \tag{2.79} \\
            \bm\Lambda_{ab} &= -(\bm\Sigma_{aa} - \bm\Sigma_{ab}\bm\Sigma_{bb}^{-1}\bm\Sigma_{ba})^{-1}\bm\Sigma_{ab}\bm\Sigma_{bb}^{-1} \tag{2.80} 
        \end{align*}
        and \vspace{-5mm}
        \begin{align*}
            \bm\mu_{a|b} &= \bm\mu_a + \bm\Sigma_{ab}\bm\Sigma_{bb}^{-1}(\mathbf x_b - \bm\mu_b) \tag{2.81} \\
            \bm\Sigma_{a|b} &= \bm\Sigma_{aa} - \bm\Sigma_{ab}\bm\Sigma_{bb}^{-1}\bm\Sigma_{ba} \tag{2.82}
        \end{align*}
    \end{frame}

    \begin{supframe}{Solution of Exercise 2.24}
    
    \end{supframe}
    
    \section{\S2.3.2 Marginal Gaussian Distribution}
    
    \begin{frame}{Derivation of Marginal Gaussian Distribution}
        \begin{block}{Marginal Gaussian Distribution}
            \[
                p(\mathbf x_a, \mathbf x_b) = \mathcal N(\mathbf x | \bm\mu, \bm\Sigma)
                \Rightarrow p(\mathbf x_a) = \mathcal N(\mathbf x_a | \bm \mu_a, \bm \Sigma_a) 
            \]
        \end{block}
        \begin{align*}
            p(\mathbf x_a) &= \int p(\mathbf x_a, \mathbf x_b) \:\mathrm d\mathbf x_b \tag{2.83}\\
            &= \int \frac{1}{(2\pi)^{D/2}|\bm\Sigma|^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu) \right\} \:\mathrm d\mathbf x_b \\
            &= \int \mathrm{const.} \cdot \exp(\redtext{\mbox{The terms involving $\mathbf x_b$}}) \cdot \exp(\bluetext{\mbox{Other terms}}) \:\mathrm d\mathbf x_b \\
            &= \mathrm{const.}\cdot \exp(\bluetext{\mbox{Other terms}}) \int \exp(\redtext{\mbox{The terms involving $\mathbf x_b$}}) \:\mathrm d\mathbf x_b
        \end{align*}
        \note{
            定義に従おう

            条件付き分布のときの議論から，指数部分が$\mathbf x_b$についての2次形式であることは明らかである．
            計算の方針は，指数部分を$\mathbf x_b$を含む項とそれ以外にわけて，積分の計算を簡単にする．
        }
    \end{frame}
    
    \begin{frame}{Derivation of Marginal Gaussian Distribution}
        \begin{align*}
            &-\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu) \\
            &=-\frac{1}{2}(\mathbf x_a - \bm\mu_a)^\top \bm\Lambda_{aa}(\mathbf x_a - \bm\mu_a)
            -\frac{1}{2}(\mathbf x_a - \bm\mu_a)^\top \bm\Lambda_{ab}(\mathbf x_b - \bm\mu_b) \\
            &\qquad -\frac{1}{2}(\mathbf x_b - \bm\mu_b)^\top \bm\Lambda_{ba}(\mathbf x_a - \bm\mu_a)
            -\frac{1}{2}(\mathbf x_b - \bm\mu_b)^\top \bm\Lambda_{bb}(\mathbf x_b - \bm\mu_b) \tag{2.70} \\
            %---------------
            &= -\frac{1}{2}\left( \mathbf x_a^\top\bm\Lambda_{aa}\mathbf x_a - \mathbf x_a^\top\bm\Lambda_{aa}\bm\mu_a - 
                               \bm\mu_a^\top\bm\Lambda_{aa}\mathbf x_a + \bm\mu_a^\top\bm\Lambda_{aa}\bm\mu_a \right) \\
            &\qquad -\frac{1}{2}\left( \highlight[red]{\mathbf x_a^\top\bm\Lambda_{ab}\mathbf x_b} - \mathbf x_a^\top\bm\Lambda_{ab}\bm\mu_b - 
                               \highlight[red]{\bm\mu_a^\top\bm\Lambda_{ab}\mathbf x_b} + \bm\mu_a^\top\bm\Lambda_{ab}\bm\mu_b \right) \\
            &\qquad -\frac{1}{2}\left( \highlight[red]{\mathbf x_b^\top\bm\Lambda_{ba}\mathbf x_a} - \highlight[red]{\mathbf x_b^\top\bm\Lambda_{ba}\bm\mu_a} - 
                               \bm\mu_b^\top\bm\Lambda_{ba}\mathbf x_a + \bm\mu_b^\top\bm\Lambda_{ba}\bm\mu_a \right) \\
            &\qquad -\frac{1}{2}\left( \highlight[red]{\mathbf x_b^\top\bm\Lambda_{bb}\mathbf x_b} - \highlight[red]{\mathbf x_b^\top\bm\Lambda_{bb}\bm\mu_b} - 
                               \highlight[red]{\bm\mu_b^\top\bm\Lambda_{bb}\mathbf x_b} + \bm\mu_b^\top\bm\Lambda_{bb}\bm\mu_b \right) \\
            &= -\frac{1}{2}\mathbf x_b^\top\bm\Lambda_{bb}\mathbf x_b + \mathbf x_b^\top \left\{ \bm\Lambda_{bb}\bm\mu_b - \bm\Lambda_{ba}(\mathbf x_a - \bm\mu_a) \right\} 
                + \mbox{\bluetext{Other terms}} \\
            &= -\frac{1}{2}\mathbf x_b^\top\bm\Lambda_{bb}\mathbf x_b + \mathbf x_b^\top \mathbf m + \mbox{\bluetext{Other terms}} \;\;\;
                (\mathbf m = \bm\Lambda_{bb}\bm\mu_b - \bm\Lambda_{ba}(\mathbf x_a - \bm\mu_a))
        \end{align*}
    \end{frame}
    
    \begin{frame}{Derivation of Marginal Gaussian Distribution}
        \vspace*{-5mm}
        \begin{align*}
            -\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu)
            &= \highlightcap[red]{\displaystyle -\frac{1}{2}\mathbf x_b^\top\bm\Lambda_{bb}\mathbf x_b + \mathbf x_b^\top \mathbf m}{The terms involving $\mathbf x_b$} + \mbox{\bluetext{Other terms}}
        \end{align*}
        \begin{block}{Completing the square}
            \begin{align*} -\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu) =  -\frac{1}{2}\mathbf x^\top\bm\Sigma^{-1}\mathbf x + \mathbf x^\top\bm\Sigma^{-1}\bm\mu + \mathrm{const.} \tag{2.71}
            \end{align*}
        \end{block}\vspace{-5mm}
        \begin{align*} &-\frac{1}{2}(\mathbf x - \bm\mu)^\top \bm\Sigma^{-1}(\mathbf x - \bm\mu) = -\frac{1}{2}\mathbf x_b^\top\bm\Lambda_{bb}\mathbf x_b + \mathbf x_b^\top \mathbf m + \mbox{\bluetext{Other terms}} \\
            =& -\frac{1}{2}\mathbf x_b^\top\highlightcap[cyan]{\bm\Lambda_{bb}}{$\bm\Sigma^{-1}$}\mathbf x_b + \mathbf x_b^\top \highlightcap[cyan]{\bm\Lambda_{bb}}{$\bm\Sigma^{-1}$}\highlightcap[green]{\bm\Lambda_{bb}^{-1}\mathbf m}{$\bm\mu$} + \mbox{\bluetext{Other terms}} \\
            =& -\frac{1}{2}(\mathbf x_b - \bm\Lambda_{bb}^{-1}\mathbf m)^\top\bm\Lambda_{bb}(\mathbf x_b - \bm\Lambda_{bb}^{-1}\mathbf m)
                +\frac{1}{2}\mathbf m^\top\bm\Lambda_{bb}^{-1}\mathbf m + \mbox{\bluetext{Other terms}} \tag{2.84'}
        \end{align*}
    \end{frame}
    
    \begin{frame}{Derivation of Marginal Gaussian Distribution}
        \begin{align*}
            p(\mathbf x_a) &= \int p(\mathbf x_a, \mathbf x_b) \:\mathrm d\mathbf x_b \tag{2.83}\\
            &= \mathrm{const.}\cdot \exp(\bluetext{\mbox{Other terms}}) \int \exp(\redtext{\mbox{The terms involving $\mathbf x_b$}}) \:\mathrm d\mathbf x_b \\
            &= \mathrm{const.}\cdot \exp(\bluetext{\mbox{Other terms}}) \cdot \exp\left( \frac{1}{2}\mathbf m^\top\bm\Lambda_{bb}^{-1}\mathbf m\right) \\
            &\qquad \cdot \int \highlightcap[cyan]{\displaystyle \exp\left\{ -\frac{1}{2}(\mathbf x_b - \bm\Lambda_{bb}^{-1}\mathbf m)^\top\bm\Lambda_{bb}(\mathbf x_b - \bm\Lambda_{bb}^{-1}\mathbf m) \right\}}{An unnormalized Gaussian (2.86)} \:\mathrm d\mathbf x_b \\
            &= \mathrm{const.}\cdot 
                \highlightcap[blue]{\displaystyle \exp(\mbox{Other terms}) \cdot \exp\left( \frac{1}{2}\mathbf m^\top\bm\Lambda_{bb}^{-1}\mathbf m\right)}{The terms involving $\mathbf x_a$}
        \end{align*}
    \end{frame}
    
    \begin{frame}{Derivation of Marginal Gaussian Distribution}
        Where,
        \begin{align*}
            \mbox{\bluetext{Other terms}} &= -\frac{1}{2}\mathbf x_a^\top\bm\Lambda_{aa}\mathbf x_a + \mathbf x_a^\top\bm\Lambda_{aa}\bm\mu_a
                + \mathbf x_a^\top\bm\Lambda_{ab}\bm\mu_b + \highlightcap[cyan]{\mathrm{const.}}{\small Independent of $\mathbf x_a$} \\
                &= -\frac{1}{2}(\mathbf x_a - \bm\mu_a)^\top\bm\Lambda_{aa}(\mathbf x_a - \bm\mu_a) + \mathbf x_a^\top\bm\Lambda_{ab}\bm\mu_b + \highlight[cyan]{\mathrm{const.}} \\
            \frac{1}{2}\mathbf m^\top\bm\Lambda_{bb}^{-1}\mathbf m &= 
                \frac{1}{2}(\mathbf x_a - \bm\mu_a)\top\bm\Lambda_{ab}\bm\Lambda_{bb}^{-1}\bm\Lambda_{ba}(\mathbf x_a - \bm\mu_a)
                - \mathbf x_a^\top\bm\Lambda_{ab}\bm\mu_b + \highlight[cyan]{\mathrm{const.}}
        \end{align*}
        \begin{align*}
            \therefore\; &\mbox{\bluetext{Other terms}} + \frac{1}{2}\mathbf m^\top\bm\Lambda_{bb}^{-1}\mathbf m \\
            &= -\frac{1}{2}(\mathbf x_a - \bm\mu_a)^\top(\bm\Lambda_{aa} - \bm\Lambda_{ab}\bm\Lambda_{bb}^{-1}\bm\Lambda_{ba})(\mathbf x_a - \bm\mu_a) + \mathrm{const.} \\
            &\Rightarrow
            \begin{cases}
                \mathbb E[\mathbf x_a] = \bm\mu_a \\
                \mathrm{cov}[\mathbf x_a] = (\bm\Lambda_{aa} - \bm\Lambda_{ab}\bm\Lambda_{bb}^{-1}\bm\Lambda_{ba})^{-1} 
            \end{cases} \tag{2.92 and 2.88}
        \end{align*}
    \end{frame}
    
    \begin{frame}{Express the results in terms of the covariance matrix}
        \begin{itemize}
            \item Recall:
                \begin{align*}
                    p(\mathbf x_a) &= \mathcal N(\mathbf x_a | \mathbb E[\mathbf x_a], \mathrm{cov}[\mathbf x_a]) \\
                    \mathbb E[\mathbf x_a] &= \bm\mu_a \tag{2.92}\\
                    \mathrm{cov}[\mathbf x_a] &= (\bm\Lambda_{aa} - \bm\Lambda_{ab}\bm\Lambda_{bb}^{-1}\bm\Lambda_{ba})^{-1} \tag{2.88}
                \end{align*}
            \item The results are expressed in terms of the partitioned precision matrix.
            \item We can also express these results in terms of the corresponding partitioned covariance matrix.
                \begin{align*}
                    \begin{pmatrix}
                        \bm \Sigma_{aa} & \bm \Sigma_{ab} \\
                        \bm \Sigma_{ba} & \bm \Sigma_{bb} 
                    \end{pmatrix}^{-1} =
                    \begin{pmatrix}
                        \bm \Lambda_{aa} & \bm \Lambda_{ab} \\
                        \bm \Lambda_{ba} & \bm \Lambda_{bb} 
                    \end{pmatrix} \tag{2.90}
                \end{align*}
        \end{itemize}
        \note{
            ここまでの議論で導かれた結果は精度行列の言葉で表現したものです．
            一方，我々はこれを元の共分散行列の言葉でも表現することが可能です．
        }
    \end{frame}

    \begin{frame}{Express the results in terms of the covariance matrix}
        \begin{block}{The identity for the inverse of a partitioned matrix (Exercise 2.24)}
            \begin{align*}
                \begin{pmatrix}
                    \mathbf A & \mathbf B \\
                    \mathbf C & \mathbf D
                \end{pmatrix}^{-1} &= 
                \begin{pmatrix}
                    \mathbf M & -\mathbf M\mathbf B\mathbf D^{-1} \\
                    -\mathbf D^{-1}\mathbf C\mathbf M & \mathbf D^{-1}+\mathbf D^{-1}\mathbf C\mathbf M \mathbf B \mathbf D^{-1}
                \end{pmatrix} \tag{2.76}\\
                \mathbf M &= (\mathbf A - \mathbf B\mathbf D^{-1}\mathbf C)^{-1} \tag{2.77}
            \end{align*}
        \end{block}
        \vspace{-0.5cm}
        \begin{align*}
            \mathrm{cov}[\mathbf x_a] &= (\bm\Lambda_{aa} - \bm\Lambda_{ab}\bm\Lambda_{bb}^{-1}\bm\Lambda_{ba})^{-1} \tag{2.88} \\
            \begin{pmatrix}
                \bm \Lambda_{aa} & \bm \Lambda_{ab} \\
                \bm \Lambda_{ba} & \bm \Lambda_{bb} 
            \end{pmatrix}^{-1}
             &= \begin{pmatrix}
                \bm \Sigma_{aa} & \bm \Sigma_{ab} \\
                \bm \Sigma_{ba} & \bm \Sigma_{bb} 
            \end{pmatrix} \tag{2.90} 
        \end{align*}
        Using (2.88) and (2.90), we obtain
        \begin{align*}
            \mathbb E[\mathbf x_a] &= \bm\mu_a \tag{2.92}\\
            \mathrm{cov}[\mathbf x_a] &= \bm\Sigma_{aa} \tag{2.93}
        \end{align*}
    \end{frame}

    \begin{frame}{Partitioned Gaussians}
        Given a joint Gaussian distribution $p(\mathbf x) = \mathcal N(\mathbf x | \bm\mu, \bm\Sigma)$ with $\bm\Lambda \equiv \bm\Sigma^{-1}$ and
        \begin{align*}
            \mathbf x = \begin{pmatrix} \mathbf x_a \\ \mathbf x_b \end{pmatrix},\quad
            &\bm\mu = \begin{pmatrix} \bm\mu_a \\ \bm\mu_b \end{pmatrix} \tag{2.94} \\
            \bm\Sigma = \begin{pmatrix}
                \bm \Sigma_{aa} & \bm \Sigma_{ab} \\
                \bm \Sigma_{ba} & \bm \Sigma_{bb} 
            \end{pmatrix},\quad
            &\bm\Lambda = \begin{pmatrix}
                \bm \Lambda_{aa} & \bm \Lambda_{ab} \\
                \bm \Lambda_{ba} & \bm \Lambda_{bb} 
            \end{pmatrix} \tag{2.95}
        \end{align*}
        Conditional distribution:
        \begin{align*}
            p(\mathbf x_a | \mathbf x_b) &= \mathcal N(\mathbf x_a | \bm\mu_{a|b}, \bm\Lambda_{aa}^{-1}) \tag{2.96} \\
            \bm\mu_{a|b} &= \bm\mu_a - \bm\Lambda_{aa}^{-1}\bm\Lambda_{ab}(\mathbf x_b - \bm\mu_b) \tag{2.97}
        \end{align*}
        Marginal distribution:
        \begin{align*}
            p(\mathbf x_a | \mathbf x_b) &= \mathcal N(\mathbf x_a | \bm\mu_a, \bm\Sigma_{aa}) \tag{2.98}
        \end{align*}
    \end{frame}
    
    \section{\S2.3.3 Bayes' theorem for Gaussian variables}
    
    \begin{frame}{Linear Gaussian Model}
        \begin{align*}
            p(\mathbf x) &= \mathcal N(\mathbf x | \bm\mu, \bm\Lambda^{-1}) \tag{2.99} \\
            p(\mathbf x | \mathbf y) &= \mathcal N(\mathbf y | \mathbf A\mathbf x + \mathbf b, \mathbf L^{-1}) \tag{2.100} \\
        \end{align*}
    \end{frame}
    
    \begin{frame}{Title}
        \begin{align*}
            \mathbf z &= \begin{pmatrix}
                \mathbf x \\ \mathbf y
            \end{pmatrix} \tag{2.101}
        \end{align*}
    \end{frame}
    
    \begin{frame}{Derivation of Joint Distribution $p(\mathbf z)$}
        From the product rule of probability, 
        \begin{align*}
            p(\mathbf z) = 
            \highlightcap[blue]{p(\mathbf x, \mathbf y)}{Unknown}
            = \highlightcap[red]{p(\mathbf y | \mathbf x)}{Known} \times \highlightcap[red]{p(\mathbf x)}{Known}
        \end{align*}
        Take logarithm of both sides, 
        \begin{align*}
            \ln p(\mathbf z) 
            &= \ln p(\mathbf x) + \ln p(\mathbf y | \mathbf x) \\
            &= -\frac{1}{2}(\mathbf x - \bm\mu)^\top\bm\Lambda(\mathbf x - \bm\mu) \\
            &\quad -\frac{1}{2}(\mathbf y - \mathbf A\mathbf x - \mathbf b)^\top\mathbf L(\mathbf y - \mathbf A\mathbf x - \mathbf b)
                + \mathrm{const.} \tag{2.102}
        \end{align*}
        Where, (2.102) is a quadratic function of $\mathbf x$ and $\mathbf y$. \par
        $\Rightarrow$ $p(\mathbf z)$ is Gaussian distribuiton. \par
        $\Rightarrow$ Completing the square!
    \end{frame}
    
    \begin{frame}{Derivation of Joint Distribution $p(\mathbf z)$}
        \begin{align*}
            \ln p(\mathbf z) 
            &= \ln p(\mathbf x) + \ln p(\mathbf y | \mathbf x) \\
            &= -\frac{1}{2}(\mathbf x - \bm\mu)^\top\bm\Lambda(\mathbf x - \bm\mu) 
            -\frac{1}{2}(\mathbf y - \mathbf A\mathbf x - \mathbf b)^\top\mathbf L(\mathbf y - \mathbf A\mathbf x - \mathbf b)
                + \mathrm{const.} \tag{2.102} \\
            &= -\frac{1}{2}(\highlight[red]{\mathbf x^\top\bm\Lambda\mathbf x} - \highlight[blue]{\mathbf x^\top\bm\Lambda\bm\mu} - \highlight[blue]{\bm\mu^\top\bm\Lambda\mathbf x} 
            + \bm\mu^\top\bm\Lambda\bm\mu) \\
            &\quad -\frac{1}{2}(\highlight[red]{\mathbf y^\top\bm\Lambda\mathbf y} - \highlight[red]{\mathbf y^\top\mathbf L\bm\Lambda\mathbf x} 
            - \highlight[blue]{\mathbf y^\top\mathbf L\mathbf b} - \highlight[red]{\mathbf x^\top\mathbf A^\top\mathbf L\mathbf y} 
            - \highlight[blue]{\mathbf b^\top\mathbf L\mathbf y} \\
            &\qquad + \highlight[red]{\mathbf x^\top\mathbf A^\top\mathbf L\mathbf A\mathbf x} + \highlight[blue]{\mathbf x^\top\mathbf A^\top\mathbf L\mathbf b}
                + \highlight[blue]{\mathbf b^\top\mathbf L\mathbf A\mathbf x} + \mathbf b^\top\mathbf L\mathbf b)+ \mathrm{const.} \\
            &= \highlightcap[red]{\displaystyle -\frac{1}{2}\mathbf x^\top(\bm\Lambda + \mathbf A^\top\mathbf L\mathbf A)\mathbf x
                -\frac{1}{2} \mathbf y^\top\mathbf L\mathbf y 
                +\frac{1}{2} \mathbf y^\top\mathbf L\mathbf A\mathbf x
                +\frac{1}{2} \mathbf x^\top\mathbf A^\top\mathbf L\mathbf y}{2.103} \\
            &\qquad + \highlightcap[blue]{\mathbf x^\top(\bm\Lambda\bm\mu - \mathbf A^\top\mathbf L\mathbf b) + \mathbf y^\top\mathbf L\mathbf b}{2.106} + \mathrm{const.}\\
        \end{align*}
    \end{frame}

    \begin{frame}{Derivation of Joint Distribution $p(\mathbf z)$}
        \vspace*{-5mm}
        \begin{align*}
            \ln p(\mathbf z) 
            &= \ln p(\mathbf x) + \ln p(\mathbf y | \mathbf x) \\
            &= -\frac{1}{2}(\mathbf x - \bm\mu)^\top\bm\Lambda(\mathbf x - \bm\mu) 
            -\frac{1}{2}(\mathbf y - \mathbf A\mathbf x - \mathbf b)^\top\mathbf L(\mathbf y - \mathbf A\mathbf x - \mathbf b)
                + \mathrm{const.} \tag{2.102} \\
            &= \highlight[red]{\displaystyle -\frac{1}{2}\mathbf x^\top(\bm\Lambda + \mathbf A^\top\mathbf L\mathbf A)\mathbf x
                -\frac{1}{2} \mathbf y^\top\mathbf L\mathbf y 
                +\frac{1}{2} \mathbf y^\top\mathbf L\mathbf A\mathbf x
                +\frac{1}{2} \mathbf x^\top\mathbf A^\top\mathbf L\mathbf y} \\
            &\qquad + \highlight[blue]{\mathbf x^\top(\bm\Lambda\bm\mu - \mathbf A^\top\mathbf L\mathbf b) + \mathbf y^\top\mathbf L\mathbf b} + \mathrm{const.}\\
            &= \begin{pmatrix}\mathbf x\\ \mathbf y\end{pmatrix}^\top
            \highlightcap[green]{\begin{pmatrix}
                \bm\Lambda + \mathbf A^\top\mathbf L\mathbf A & -\mathbf A^\top\mathbf L \\
                -\mathbf L\mathbf A& \mathbf L
            \end{pmatrix}}{$\mathbf R$ (Precision mat)}
            \begin{pmatrix}\mathbf x\\ \mathbf y\end{pmatrix}
            + \begin{pmatrix}\mathbf x\\ \mathbf y\end{pmatrix}^\top
                \highlightcap[green]{\begin{pmatrix}\bm\Lambda\bm\mu - \mathbf A^\top\mathbf L\mathbf b \\ \mathbf L\mathbf b\end{pmatrix}}{$\mathbf m$ (mean vec)} \\
            & \quad + \mathrm{const.} \\
            &= \mathbf z^\top\mathbf R\mathbf z + \mathbf z^\top \mathbf m + \mathrm{const.} \\
            &= \mathbf z^\top\mathbf R\mathbf z + \mathbf z^\top \mathbf R \mathbf R^{-1} \mathbf m + \mathrm{const.} \Longrightarrow \mbox{Completing the square!}
        \end{align*}
    \end{frame}
    
    \section{\S2.3.4 Maximum likelihood for the Gaussian}

    \begin{frame}{Maximum likelihood estimation for the Gaussian}
        Maximize the likelihood with regard to $\bm\mu$ and $\bm\Sigma^{-1}$ (i.e. precision $\bm\Lambda$)
        \begin{align*}
            p(\mathbf X | \bm\mu, \bm\Sigma) = \prod_{n=1}^N
            \frac{1}{(2\pi)^{D/2}}\frac{1}{|\bm\Sigma|^{1/2}}
            \exp\left( -\frac{1}{2}(\mathbf x_n - \bm\mu)^\top\bm\Sigma^{-1}(\mathbf x_n - \bm\mu) \right)
        \end{align*}
        By taking logarithm, we obtain:
        \begin{align*}
            \ln p(\mathbf X | \bm\mu, \bm\Sigma) = 
            -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln |\bm\Sigma|
            -\frac{1}{2}\sum_{n=1}^N (\mathbf x_n - \bm\mu)^\top\bm\Sigma^{-1}(\mathbf x_n - \bm\mu)
            \tag{2.118}
        \end{align*}
        We see that the log-likelihood depends on the dataset only the quantities (the proof is next slide):
        \begin{align*}
            \sum_{n=1}^{N} \mathbf x_n \;\;\;\;\;\;\;\;\;\;
            \sum_{n=1}^{N} \mathbf x_n\mathbf x_n^\top
            \tag{2.119}
        \end{align*}
        These are know as the \textit{sufficient statistics} for  the Gauss.

        \note{
            尤度関数の中のデータ集合への依存は次の2つの量のみ
        }
    \end{frame}

    \begin{supframe}{Proof: the log-likelihood depends on the dataset only the sufficient stats}
        \begin{align*}
            &-\frac{1}{2}\sum_{n=1}^N (\mathbf x_n - \bm\mu)^\top\bm\Sigma^{-1}(\mathbf x_n - \bm\mu) \\
            &=-\frac{1}{2}\sum_{n=1}^N \mathbf x_n^\top\bm\Sigma^{-1}\mathbf x_n
            + \left( \sum_{n=1}^N \mathbf x_n \right)^\top\bm\Sigma^{-1}\bm\mu
            -\frac{N}{2}\bm\mu^\top\bm\Sigma^{-1}\bm\mu \\
            &=-\frac{1}{2}\highlightcap[red]{\displaystyle\tr\left(\bm\Sigma^{-1}\sum_{n=1}^N \mathbf x_n\mathbf x_n^\top\right)}{(C.9) and linearity of trace}
            + \left( \sum_{n=1}^N \mathbf x_n \right)^\top\bm\Sigma^{-1}\bm\mu
            -\frac{N}{2}\bm\mu^\top\bm\Sigma^{-1}\bm\mu \\
            &=-\frac{1}{2}\tr\nbracket{\bm\Sigma^{-1} \abracket{\mathbf x\mathbf x^\top}}
            + \abracket{\mathbf x}^\top\bm\Sigma^{-1}\bm\mu
            -\frac{N}{2}\bm\mu^\top\bm\Sigma^{-1}\bm\mu
        \end{align*}
        Where, 
        \begin{align*}
            \abracket{\mathbf x} \equiv \sum_{n=1}^{N} \mathbf x_n \;\;\;\;\;\;\;\;
            \abracket{\mathbf x\mathbf x^\top} \equiv \sum_{n=1}^{N} \mathbf x_n\mathbf x_n^\top
        \end{align*}
    \end{supframe}

    \begin{frame}{Derivation of ML solutions}
        The log-likelihood (definition of notation is prev slide):
        \begin{align*}
            \ln p(\mathbf X | \bm\mu, \bm\Sigma) = 
            -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln |\bm\Sigma|
            -\frac{1}{2}\tr\nbracket{\bm\Sigma^{-1} \abracket{\mathbf x\mathbf x^\top}} \\
            \quad 
            + \abracket{\mathbf x}^\top\bm\Sigma^{-1}\bm\mu
            -\frac{N}{2}\bm\mu^\top\bm\Sigma^{-1}\bm\mu
            \tag{2.118'}
        \end{align*}
        By setting the derivative of the log-likelihood with respect to $\bm\mu$ and $\bm\Sigma^{-1}$ to zero, like
        \begin{align*}
            \frac{\partial}{\partial \bm\mu}\ln p(\mathbf X|\bm\mu, \bm\Sigma) = \mathbf 0 \;\;\;\;\;\;\;
            \frac{\partial}{\partial \bm\Sigma^{-1}}\ln p(\mathbf X|\bm\mu, \bm\Sigma) = O 
        \end{align*}
        we obtain the solution for ML given by:
        \begin{align*}
            \bm\mu_\mathrm{ML} &= \frac{1}{N}\abracket{\mathbf x} = \frac{1}{N} \sum_{n=1}^N \mathbf x_n \tag{2.121} \\
            \bm\Sigma_\mathrm{ML} &= \frac{1}{N}\abracket{\mathbf x\mathbf x^\top} - \bm\mu\bm\mu^\top
            = \frac{1}{N} \sum_{n=1}^N (\mathbf x_n - \bm\mu_\mathrm{ML})(\mathbf x_n - \bm\mu_\mathrm{ML})^\top \tag{2.122} \\
        \end{align*}
    \end{frame}

    \begin{supframe}{Derivation of the ML solution $\bm\mu_\mathrm{ML}$}

    \end{supframe}
    
    \begin{supframe}{Derivation of the ML solution $\bm\Sigma_\mathrm{ML}$}

    \end{supframe}

    \begin{frame}{Unbiased estimator}
        By evaluating $\mathbb E[\bm\mu_\mathrm{ML}]$ and $\mathbb E[\bm\Sigma_\mathrm{ML}]$ under the true distribution, 
        we obtain:
        \begin{align*}
            \mathbb E[\bm\mu_\mathrm{ML}] &= \bm\mu \tag{2.123} \\
            \mathbb E[\bm\Sigma_\mathrm{ML}] &= \frac{N-1}{N}\bm\Sigma \tag{2.124}
        \end{align*}
        We can correct the bias of $\mathbb E[\bm\Sigma_\mathrm{ML}]$ by defining
        \begin{align*}
            \widetilde{\bm\Sigma} &= \frac{1}{N-1}\sum_{n=1}^N (\mathbf x_n - \bm\mu_\mathrm{ML})(\mathbf x_n - \bm\mu_\mathrm{ML})^\top \tag{2.125} \\
            &\Rightarrow \;\; \mathbb E[\widetilde{\bm\Sigma}] = \bm\Sigma
        \end{align*}
    \end{frame}
    
    \begin{supframe}{Solution of Ex.(2.35)}

    \end{supframe}

    \begin{frame}{Title}
        Hello metropolis!
    \end{frame}

    \section{\S2.3.5 Sequential estimation}

    \begin{frame}{Title}
        Hello metropolis!
    \end{frame}

    \begin{frame}{Title}
        Hello metropolis!
    \end{frame}

    \begin{frame}{Title}
        Hello metropolis!
    \end{frame}

    \section{\S2.3.6 Bayesian inference for the Gaussian}

    \begin{frame}{Maximum Likelihood Estimation vs. Bayesian Inference}
        Hello metropolis!
    \end{frame}

    \begin{frame}{The task of inferring the mean $\mu$ (the variance $\sigma^2$ is known)}
        \begin{itemize}
            \item A set of $N$ observations: $\mathbf X = \cbracket{x_1, \cdots, x_N}$
            \item The likelihood function (the prob. of the observed data given $\mu$):
                \begin{align*}
                    p(\mathbf X | \mu) = \prod_{n=1}^N p(x_n | \mu)
                    = \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\cbracket{-\frac{1}{2\sigma^2}\sum_{n=1}^N (x_n - \mu)^2} \tag{2.137}
                \end{align*}
            \item NOTE: $p(\mathbf X | \mu)$ is not a prob dist over $\mu$ and not normalized.
            \item By introducing a prior $p(\mu)$, the posterior given by
                \begin{align*}
                    p(\mu | \mathbf X) \propto p(\mathbf X | \mu) p(\mu) \tag{2.139}
                \end{align*}
            \item What prior $p(\mu)$ should we choose?
        \end{itemize}
    \end{frame}

    \begin{frame}{The task of inferring the mean $\mu$ (the variance $\sigma^2$ is known)}
        \begin{itemize}
            \item Recall:
                \begin{align*}
                    p(\mathbf X | \mu) = \prod_{n=1}^N p(x_n | \mu)
                    = \frac{1}{(2\pi\sigma^2)^{N/2}}
                    \highlightcap[red]{\displaystyle\exp\cbracket{-\frac{1}{2\sigma^2}\sum_{n=1}^N (x_n - \mu)^2}}{The exp of a quadratic form in $\mu$} \tag{2.137}
                \end{align*}
            \item The likelihood takes the form of the exp of a quadratic form in $\mu$.
            \item Thus, if we choose a Gaussian as the prior, the posterior will also be Gaussian.
            \item We therefore take our prior to be 
                \begin{align*}
                    p(\mu) = \mathcal N(\mu | \mu_0, \sigma_0^2). \tag{2.138}
                \end{align*}
        \end{itemize}
    \end{frame}
    
    \begin{frame}{The task of inferring the mean $\mu$ (the variance $\sigma^2$ is known)}
        \begin{itemize}
            \item The likelihood function:
                \begin{align*}
                    p(\mathbf X | \mu) &= \prod_{n=1}^N p(x_n | \mu)
                    = \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\cbracket{-\frac{1}{2\sigma^2}\sum_{n=1}^N (x_n - \mu)^2} \tag{2.137}
                \end{align*}
            \item The prior distribution: $p(\mu) = \mathcal N(\mu | \mu_0, \sigma_0^2)$ \hfill (2.138)
            \item By using $p(\mu | \mathbf X) \propto p(\mathbf X | \mu) p(\mu)$ (2.139) and normalizing it, \par, 
                we obtain the posterior (derivation: ex. 2.38):
                \begin{align*}
                    p(\mu | \mathbf X) &= \mathcal N(\mu | \mu_N, \sigma_N^2) \tag{2.140} \\
                    \mu_N &= \frac{\sigma^2}{N\sigma_0^2 + \sigma}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma}\mu_\mathrm{ML} \tag{2.141} \\
                    \frac{1}{\sigma_N^2} &= \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2} \tag{2.142}
                \end{align*}
        \end{itemize}
    \end{frame}

    \begin{supframe}{Solution of ex. 2.38}

    \end{supframe}
    
    \begin{frame}{Interpretation of the posterior's params}
        \begin{itemize}
            \item Recall:
                \begin{align*}
                    p(\mu | \mathbf X) &= \mathcal N(\mu | \mu_N, \sigma_N^2) \tag{2.140} \\
                    \mu_N &= \frac{\sigma^2}{N\sigma_0^2 + \sigma}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma}\mu_\mathrm{ML} \tag{2.141} \\
                    \frac{1}{\sigma_N^2} &= \frac{1}{\sigma_0^2} + \frac{N}{\sigma^2} \tag{2.142}
                \end{align*}
        \end{itemize}
    \end{frame}
    
    \begin{frame}{Sequential estimation with the Bayesian paradigm}
        \begin{align*}
            p(\mu | \mathbf X) &\propto p(\mu)\prod_{n=1}^{N}p(x_n | \mu) \\
            &= \highlightcap<2->[red]{\displaystyle\rbracket{p(\mu)\prod_{n=1}^{N-1}p(x_n | \mu)}}{The posterior after \\ observing $N-1$ data}
            \highlightcap<2->[blue]{p(x_N | \mu)}{The likelihood \\ with $N$-th data} \tag{2.144}
        \end{align*}
        \vspace{-7.5mm}
        \begin{figure}[h]\centering
            \includegraphics[height=4.0cm]{./figs/Figure2_12.pdf}\hspace{1em}Figure 2.12
        \end{figure}
    \end{frame}
    
    \begin{frame}{The task of inferring the precision $\lambda$ (the mean $\mu$ is known)}
        Hello metropolis!
    \end{frame}

    \begin{frame}{Title}
        Hello metropolis!
    \end{frame}

    \section{\S2.3.7 Student's t-distribution}

    \begin{frame}{Title}
        Hello metropolis!
    \end{frame}

\end{document}
